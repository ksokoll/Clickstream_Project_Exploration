{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHuv8P4rZ3TdXYIAZGUgg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksokoll/Clickstream_Project_Exploration/blob/main/clickstream_project_static_anomaly_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clickstream Anomaly Detection - Static Analysis\n",
        "\n",
        "This script analyzes the complete historical dataset to identify conversion rate anomalies.\n",
        "\n",
        "Each step of the analysis is wrapped in a function instead of keeping it plain in a code-cell, as in the last step (main()) there will be a loop to conduct the analysis for various different combinations of device, os and browser.\n",
        "\n",
        "Each step is explained in detail, especially the parts where I use a different metric than usual."
      ],
      "metadata": {
        "id": "VwbiGqFtwOEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os"
      ],
      "metadata": {
        "id": "Qk1CvxxawN1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98sur_j3v4OR"
      },
      "outputs": [],
      "source": [
        "INPUT_DIR = '/content/files'\n",
        "SIGMA_THRESHOLD = 2  # +/-2 sigma\n",
        "CONSECUTIVE_DAYS = 3  # Alert after 3 consecutive anomalies\n",
        "BASELINE_WEEKS = 4   # Use last 4 weeks for baseline calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used 2 sigma here, as I tested different combinations (1-3 sigma) manually. +/- 2 sigma proved to be the best value to detect only the correct positive values without skipping or throwing too much false alarms."
      ],
      "metadata": {
        "id": "9y2QKVImyHFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets read the CSV into a dataframe:"
      ],
      "metadata": {
        "id": "XKB8LBk4WanY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load input data"
      ],
      "metadata": {
        "id": "r1TpRUjB1pzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(INPUT_DIR+\"/df_contaminated.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "NB1T6AcxwoMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcf6678-70e7-45d5-b742-99f93076968d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       timestamp  visitorid event  itemid  transactionid   device browser  \\\n",
            "0  1433221332117     257597  view  355908            NaN  desktop  chrome   \n",
            "1  1433224214164     992329  view  248676            NaN   mobile  safari   \n",
            "2  1433221999827     111016  view  318965            NaN   mobile  chrome   \n",
            "3  1433221955914     483717  view  253185            NaN  desktop  chrome   \n",
            "4  1433221337106     951259  view  367447            NaN   mobile  chrome   \n",
            "\n",
            "        os       timestamp_readable  \n",
            "0  windows  2015-06-02 05:02:12.117  \n",
            "1      ios  2015-06-02 05:50:14.164  \n",
            "2  android  2015-06-02 05:13:19.827  \n",
            "3    macos  2015-06-02 05:12:35.914  \n",
            "4  android  2015-06-02 05:02:17.106  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate_daily_conversion"
      ],
      "metadata": {
        "id": "XMefMnZP1oAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use **calculate_daily_conversion** for two important things:\n",
        "\n",
        "1. Aggregating the dataframe: As you can see above, each day of the whole period is spread across single event line items. Since we are interested in the aggregated numbers, we do it first.\n",
        "\n",
        "2. calculate the conversion rates ourselves by applying: transactions/vievs*100. It is not part of the original dataset."
      ],
      "metadata": {
        "id": "Z4w5xo6wyox5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_daily_conversion(df, browser, os, device):\n",
        "    \"\"\"\n",
        "    Calculate daily conversion rate for specific browser/OS/device combination\n",
        "    Conversion Rate = Transactions / Views\n",
        "    \"\"\"\n",
        "    print(f\"Calculating daily conversion for {device}/{os}/{browser}...\")\n",
        "\n",
        "    filtered = df[\n",
        "    (df['browser'] == browser) &\n",
        "    (df['os'] == os) &\n",
        "    (df['device'] == device)\n",
        "    ].copy()\n",
        "\n",
        "    # convert date column\n",
        "    filtered['date'] = pd.to_datetime(filtered['timestamp_readable']).dt.date\n",
        "\n",
        "    # Count views per day\n",
        "    views_per_day = filtered[filtered['event'] == 'view'].groupby('date').size()\n",
        "\n",
        "    # Count transactions per day\n",
        "    trans_per_day = filtered[filtered['event'] == 'transaction'].groupby('date').size()\n",
        "\n",
        "    # Combine into DataFrame\n",
        "    daily = pd.DataFrame({\n",
        "    'views': views_per_day,\n",
        "    'transactions': trans_per_day\n",
        "    }).fillna(0).reset_index()\n",
        "\n",
        "    daily['conversion_rate'] = (\n",
        "    daily['transactions'] / daily['views'].replace(0, 1)\n",
        "    ) * 100\n",
        "\n",
        "    print(f\"Calculated conversion for {len(daily)} days\")\n",
        "    return daily"
      ],
      "metadata": {
        "id": "Xefc_VVvwqg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the evolution from event line items to aggregated days which shrinked the whole data table from ~ 2 million rows to only 107:"
      ],
      "metadata": {
        "id": "0T8YIFQz0nEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "browser = 'chrome'\n",
        "os = 'windows'\n",
        "device = 'desktop'\n",
        "daily_conversion = calculate_daily_conversion(df, browser, os, device)\n",
        "print(daily_conversion.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnPy2UVU0ohc",
        "outputId": "3b614eec-79be-45af-8bda-3539008504bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating daily conversion for desktop/windows/chrome...\n",
            "Calculated conversion for 107 days\n",
            "         date  views  transactions  conversion_rate\n",
            "0  2015-05-03   2946            17         0.577054\n",
            "1  2015-05-04   4084            34         0.832517\n",
            "2  2015-05-05   4825            47         0.974093\n",
            "3  2015-05-06   5295            65         1.227573\n",
            "4  2015-05-07   5118            56         1.094177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "daily_conversion.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF-3WV8D1HqT",
        "outputId": "6e8429cc-93a8-4591-852f-9bae0c94e9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 107 entries, 0 to 106\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   date             107 non-null    object \n",
            " 1   views            107 non-null    int64  \n",
            " 2   transactions     107 non-null    int64  \n",
            " 3   conversion_rate  107 non-null    float64\n",
            "dtypes: float64(1), int64(2), object(1)\n",
            "memory usage: 3.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate_baseline_and_sigma"
      ],
      "metadata": {
        "id": "rdLP2ELh1kxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue now by adding two new calculated columns to our dataframe: **baseline** and **sigma**:\n",
        "\n",
        "**baseline**: The baseline takes the conversion rates of the last n consecutive same weekdays to calculate something we can measure potential anomalies against. After a bit of back and forth with mean() and median(), which were either too sensitive or threw too many false positives, I settled with **Median Absolute Deviation (MAD)**. This metric is better at handling outliers (extreme values). As an example:\n",
        "\n",
        "If the last 4 Sundays have the conversion rates [0.8%, 0.75%, 0.8%, 0.1%], the \"0.1%\" datapoint imposes a problem, as the mean() will be 0.612% despite three of four values being quite high (>0.75%)!\n",
        "\n",
        "**With MAD, the calculation is as follows:**\n",
        "\n",
        "**Step 1:** Calculate the median (our baseline):\n",
        "```\n",
        "median([0.1, 0.75, 0.8, 0.8]) = (0.75 + 0.8) / 2 = 0.775%\n",
        "```\n",
        "\n",
        "**Step 2:** For each value, calculate the absolute deviation from the median:\n",
        "```\n",
        "|0.1 - 0.775| = 0.675\n",
        "|0.75 - 0.775| = 0.025\n",
        "|0.8 - 0.775| = 0.025\n",
        "|0.8 - 0.775| = 0.025\n",
        "```\n",
        "\n",
        "This gives us the list of absolute deviations:\n",
        "```\n",
        "[0.025, 0.025, 0.025, 0.675]\n",
        "```\n",
        "\n",
        "**Step 3:** Take the median of these deviations (this is MAD):\n",
        "```\n",
        "MAD = median([0.025, 0.025, 0.025, 0.675]) = (0.025 + 0.025) / 2 = 0.025\n",
        "```\n",
        "\n",
        "**Step 4:** Scale MAD to be comparable with standard deviation:\n",
        "```\n",
        "sigma = MAD c 1.4826 = 0.025 c 1.4826 = 0.037\n",
        "```\n",
        "\n",
        "The scaling factor **1.4826** comes from the mathematical relationship between MAD and standard deviation in a normal distribution. This allows us to continue using \"±2 sigma\" threshold logic while benefiting from MAD's robustness.\n",
        "\n",
        "**The big advantage:** MAD is robust against extreme deviations because it uses absolute values and medians, whereas standard deviation squares deviations (making outliers disproportionately large). This prevents our threshold from becoming unrealistically wide when contaminated data points exist in the lookback window, which is crucial for accurate anomaly detection.\n",
        "\n",
        "Other methods for handling these outliers would have been trimming or winsorizing our observations, which would have been more complex an in the need of importing more dependancies, for example from sklearn, and MAD really fits our purpose here."
      ],
      "metadata": {
        "id": "_xdf8Ki017uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_baseline_and_sigma(daily_data, weeks=BASELINE_WEEKS):\n",
        "    \"\"\"\n",
        "    Calculate baseline conversion rate using same-weekday average from last N weeks.\n",
        "    Also calculates standard deviation for anomaly threshold.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"Calculating baseline (last {weeks} weeks, same weekday)...\")\n",
        "\n",
        "    # Add weekday column (0=Monday, 6=Sunday)\n",
        "    daily_data = daily_data.copy()\n",
        "    daily_data['date'] = pd.to_datetime(daily_data['date'])\n",
        "    daily_data['weekday'] = daily_data['date'].dt.weekday\n",
        "\n",
        "     # Sort by date\n",
        "    daily_data = daily_data.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # Calculate baseline for each day\n",
        "    baselines = []\n",
        "    sigmas = []\n",
        "\n",
        "    for idx, row in daily_data.iterrows():\n",
        "        current_date = row['date']\n",
        "        current_weekday = row['weekday']\n",
        "\n",
        "        # Get dates from last N weeks with same weekday\n",
        "        lookback_start = current_date - pd.Timedelta(days=weeks*7)\n",
        "\n",
        "        # Find same weekdays in lookback period\n",
        "        same_weekdays = daily_data[\n",
        "            (daily_data['weekday'] == current_weekday) &\n",
        "            (daily_data['date'] < current_date) &\n",
        "            (daily_data['date'] >= lookback_start)\n",
        "        ]\n",
        "\n",
        "        # Calculate baseline (mean) and sigma (std)\n",
        "        if len(same_weekdays) >= 2:  # Need at least 2 data points\n",
        "            baseline = same_weekdays['conversion_rate'].median()\n",
        "            mad = (same_weekdays['conversion_rate'] - baseline).abs().median()\n",
        "            sigma = mad * 1.4826\n",
        "        else:\n",
        "            # Not enough data in the beginning - use overall stats\n",
        "            baseline = daily_data['conversion_rate'].median()\n",
        "            mad = (daily_data['conversion_rate'] - baseline).abs().median()\n",
        "            sigma = mad * 1.4826\n",
        "\n",
        "        baselines.append(baseline)\n",
        "        sigmas.append(sigma)\n",
        "\n",
        "    # Add to dataframe\n",
        "    daily_data['baseline'] = baselines\n",
        "    daily_data['sigma'] = sigmas\n",
        "\n",
        "    print(f\"Calculated baselines for {len(daily_data)} days\")\n",
        "\n",
        "    return daily_data"
      ],
      "metadata": {
        "id": "5lFRZGrmwsmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_data = calculate_baseline_and_sigma(daily_conversion, BASELINE_WEEKS)\n",
        "print(baseline_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEZboCk01Y0H",
        "outputId": "9633392e-5c33-40db-f1e1-19e1b7f527cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "        date  views  transactions  conversion_rate  weekday  baseline  \\\n",
            "0 2015-05-03   2946            17         0.577054        6  0.938158   \n",
            "1 2015-05-04   4084            34         0.832517        0  0.938158   \n",
            "2 2015-05-05   4825            47         0.974093        1  0.938158   \n",
            "3 2015-05-06   5295            65         1.227573        2  0.938158   \n",
            "4 2015-05-07   5118            56         1.094177        3  0.938158   \n",
            "\n",
            "      sigma  \n",
            "0  0.365477  \n",
            "1  0.365477  \n",
            "2  0.365477  \n",
            "3  0.365477  \n",
            "4  0.365477  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# detect_anomalies"
      ],
      "metadata": {
        "id": "iNom_NiE1tIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step a quite simple comparison of the \"conversion_rate\" of the current day in comparison to the baseline is performed, to measure if the conversion rate is distinctevly low. Note: With \"threshold\" we are looking only onto the lower threshold, as the stakeholder are particularly interested in drops in conversion rate as opposed to a higher conversion rate than usual, which might also be interesting for other purposes but is out of scope here.\n",
        "\n",
        "Here is a detailled explanation based on a single data point:\n",
        "\n",
        "**date**: 2015-05-03 (which is a sunday)\n",
        "\n",
        "**conversion rate:** 0,577% (measured conversion rate TODAY)\n",
        "\n",
        "**baseline:** 0,938% (the \"regular\" conversion rate based on the last X sundays)\n",
        "\n",
        "**sigma**: 0,365 (typical variability)\n",
        "\n",
        "**threshold**: baseline - 2x sigma = 0,938 - 2x(0,365) = 0,208\n",
        "\n",
        "**is_anomaly**: False\n",
        "\n"
      ],
      "metadata": {
        "id": "mou1-yHVaCsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(baseline_data, sigma_threshold=SIGMA_THRESHOLD):\n",
        "    \"\"\"\n",
        "    Detect anomalies where conversion < (baseline - sigma_threshold * sigma)\n",
        "    \"\"\"\n",
        "    print(f\"Detecting anomalies (threshold: {sigma_threshold} sigma)...\")\n",
        "\n",
        "    df = baseline_data.copy()\n",
        "\n",
        "    # Calculate threshold\n",
        "    df['threshold'] = df['baseline'] - (sigma_threshold * df['sigma'])\n",
        "\n",
        "    # Clip to avoid negative thresholds\n",
        "    df['threshold'] = df['threshold'].clip(lower=0)\n",
        "\n",
        "    # Flag anomalies\n",
        "    df['is_anomaly'] = df['conversion_rate'] <= df['threshold']\n",
        "\n",
        "    # Calculate deviation in sigma units\n",
        "    df['deviation_sigma'] = (df['conversion_rate'] - df['baseline']) / df['sigma']\n",
        "\n",
        "    # Count anomalies\n",
        "    anomaly_count = df['is_anomaly'].sum()\n",
        "    print(f\"Found {anomaly_count} anomalous days\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "YO_D_O3DwuDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(detect_anomalies(baseline_data, sigma_threshold=SIGMA_THRESHOLD).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ILMtzCaU1m",
        "outputId": "7cd203e5-1c05-494f-8644-fae99217602a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 7 anomalous days\n",
            "        date  views  transactions  conversion_rate  weekday  baseline  \\\n",
            "0 2015-05-03   2946            17         0.577054        6  0.938158   \n",
            "1 2015-05-04   4084            34         0.832517        0  0.938158   \n",
            "2 2015-05-05   4825            47         0.974093        1  0.938158   \n",
            "3 2015-05-06   5295            65         1.227573        2  0.938158   \n",
            "4 2015-05-07   5118            56         1.094177        3  0.938158   \n",
            "\n",
            "      sigma  threshold  is_anomaly  deviation_sigma  \n",
            "0  0.365477   0.207204       False        -0.988036  \n",
            "1  0.365477   0.207204       False        -0.289050  \n",
            "2  0.365477   0.207204       False         0.098324  \n",
            "3  0.365477   0.207204       False         0.791883  \n",
            "4  0.365477   0.207204       False         0.426892  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# find_consecutive_alerts"
      ],
      "metadata": {
        "id": "zxjBUSIC1ug6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our calculated column \"is_anomaly\" we can observe where streaks of anomalies happen, or cluster. The logic is straightforward: If there are X consecutive anomalies, the period is flagged as critical."
      ],
      "metadata": {
        "id": "Igg-XTsocpCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_consecutive_alerts(anomalies_df, consecutive_days=3):\n",
        "    \"\"\"Find periods where anomalies occur N consecutive days\"\"\"\n",
        "    print(f\"Finding {consecutive_days}-day consecutive anomalies...\")\n",
        "\n",
        "    df = anomalies_df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    critical_periods = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(df):\n",
        "        if df.iloc[i]['is_anomaly']:\n",
        "            # Start of potential streak\n",
        "            start_idx = i\n",
        "            count = 1\n",
        "\n",
        "            # Count consecutive anomalies\n",
        "            while i + 1 < len(df) and df.iloc[i + 1]['is_anomaly']:\n",
        "                count += 1\n",
        "                i += 1\n",
        "\n",
        "            # Check if streak is long enough\n",
        "            if count >= consecutive_days:\n",
        "                start_date = df.iloc[start_idx]['date']\n",
        "                end_date = df.iloc[i]['date']\n",
        "                critical_periods.append((start_date, end_date, count))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Found {len(critical_periods)} critical periods (≥{consecutive_days} days)\")\n",
        "\n",
        "    return critical_periods"
      ],
      "metadata": {
        "id": "oAlnd-4_wzE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print_report"
      ],
      "metadata": {
        "id": "tTmyjxnF1v5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get a report of our results."
      ],
      "metadata": {
        "id": "_fN34nyic_4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_report(anomalies_df, critical_periods, browser, os, device):\n",
        "    \"\"\"Print formatted terminal report\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"ANOMALY DETECTION REPORT -  {browser} {os} {device}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Summary Statistics\n",
        "    total_days = len(anomalies_df)\n",
        "    anomaly_days = anomalies_df['is_anomaly'].sum()\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"   Total days:       {total_days}\")\n",
        "    print(f\"   Anomalous days:   {anomaly_days} ({anomaly_days/total_days*100:.1f}%)\")\n",
        "    print(f\"   Critical periods: {len(critical_periods)}\")\n",
        "\n",
        "    # Critical Periods\n",
        "    if critical_periods:\n",
        "        print(f\"\\nCritical Periods (≥3 consecutive days):\")\n",
        "        for i, (start, end, length) in enumerate(critical_periods, 1):\n",
        "            print(f\"   {i}. {start} → {end} ({length} days)\")"
      ],
      "metadata": {
        "id": "vKvKlyw1w14D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# export_csv"
      ],
      "metadata": {
        "id": "aSvZUXCK1yZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export so the values can be saved."
      ],
      "metadata": {
        "id": "YVCPPBXldHVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_csv(anomalies_df, output_path, os_, browser, device):\n",
        "    \"\"\"Export results to CSV for Power BI integration\"\"\"\n",
        "    print(f\"\\nExporting results to {output_path}...\")\n",
        "\n",
        "    # Select relevant columns for Power BI\n",
        "    export_df = anomalies_df[[\n",
        "        'date',\n",
        "        'views',\n",
        "        'transactions',\n",
        "        'conversion_rate',\n",
        "        'baseline',\n",
        "        'sigma',\n",
        "        'threshold',\n",
        "        'deviation_sigma',\n",
        "        'is_anomaly'\n",
        "    ]].copy()\n",
        "\n",
        "    # Convert date to string for CSV\n",
        "    export_df['date'] = export_df['date'].astype(str)\n",
        "\n",
        "    # Save to CSV\n",
        "    export_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Exported {len(export_df)} rows to {output_path}\")\n"
      ],
      "metadata": {
        "id": "h-yXCJmgw4fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main()"
      ],
      "metadata": {
        "id": "9hfOi2aW1z5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main now combines all of the above step to move through the whole analysis.\n",
        "\n",
        "Noteworthy is that we do a iteration for each interesting combination of os x browser x device to really make sure that nothing is slipping through the net.\n",
        "\n",
        "We did not to each iteration (which would be nine), as some of them do not make sense. The static \"COMBINATIONS_TO_CHECK\" variable allows later changes, as the customer might notice that a combination can be dropped or new devices or browsers should be observed. This can later be managed by environment variables."
      ],
      "metadata": {
        "id": "iaVxCYGCdOlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  COMBINATIONS_TO_CHECK = [\n",
        "  ('safari', 'ios', 'mobile'),\n",
        "  ('safari', 'macos', 'desktop'),\n",
        "  ('chrome', 'android', 'mobile'),\n",
        "  ('chrome', 'windows', 'desktop'),\n",
        "  ('firefox', 'windows', 'desktop'),\n",
        "]\n",
        "\n",
        "  for browser, os, device in COMBINATIONS_TO_CHECK:\n",
        "      output_path = f\"data/output_{device}_{os}_{browser}.csv\"\n",
        "\n",
        "  # Step 2: Calculate daily conversion\n",
        "      daily_conversion = calculate_daily_conversion(df, browser, os, device)\n",
        "\n",
        "      # Step 3: Calculate baseline & sigma\n",
        "      baseline_data = calculate_baseline_and_sigma(daily_conversion, BASELINE_WEEKS)\n",
        "\n",
        "      # Step 4: Detect anomalies\n",
        "      anomalies = detect_anomalies(baseline_data, SIGMA_THRESHOLD)\n",
        "\n",
        "      # Step 5: Find consecutive streaks\n",
        "      critical_periods = find_consecutive_alerts(anomalies, CONSECUTIVE_DAYS)\n",
        "\n",
        "      # Step 6: Print report\n",
        "      print_report(anomalies, critical_periods, browser, os, device)\n",
        "\n",
        "      # Step 7: Export CSV\n",
        "      #export_csv(anomalies, output_path, browser, os, device)\n",
        "\n",
        "  print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV6DpZmkw6Y7",
        "outputId": "36b4ba3a-bd02-4df9-f7cc-d838a09f3b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating daily conversion for mobile/ios/safari...\n",
            "Calculated conversion for 107 days\n",
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 24 anomalous days\n",
            "Finding 3-day consecutive anomalies...\n",
            "Found 2 critical periods (≥3 days)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION REPORT -  safari ios mobile\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "   Total days:       107\n",
            "   Anomalous days:   24 (22.4%)\n",
            "   Critical periods: 2\n",
            "\n",
            "Critical Periods (≥3 consecutive days):\n",
            "   1. 2015-09-01 00:00:00 → 2015-09-12 00:00:00 (12 days)\n",
            "   2. 2015-09-16 00:00:00 → 2015-09-18 00:00:00 (3 days)\n",
            "Calculating daily conversion for desktop/macos/safari...\n",
            "Calculated conversion for 107 days\n",
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 16 anomalous days\n",
            "Finding 3-day consecutive anomalies...\n",
            "Found 0 critical periods (≥3 days)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION REPORT -  safari macos desktop\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "   Total days:       107\n",
            "   Anomalous days:   16 (15.0%)\n",
            "   Critical periods: 0\n",
            "Calculating daily conversion for mobile/android/chrome...\n",
            "Calculated conversion for 107 days\n",
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 6 anomalous days\n",
            "Finding 3-day consecutive anomalies...\n",
            "Found 0 critical periods (≥3 days)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION REPORT -  chrome android mobile\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "   Total days:       107\n",
            "   Anomalous days:   6 (5.6%)\n",
            "   Critical periods: 0\n",
            "Calculating daily conversion for desktop/windows/chrome...\n",
            "Calculated conversion for 107 days\n",
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 7 anomalous days\n",
            "Finding 3-day consecutive anomalies...\n",
            "Found 0 critical periods (≥3 days)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION REPORT -  chrome windows desktop\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "   Total days:       107\n",
            "   Anomalous days:   7 (6.5%)\n",
            "   Critical periods: 0\n",
            "Calculating daily conversion for desktop/windows/firefox...\n",
            "Calculated conversion for 107 days\n",
            "Calculating baseline (last 4 weeks, same weekday)...\n",
            "Calculated baselines for 107 days\n",
            "Detecting anomalies (threshold: 2 sigma)...\n",
            "Found 16 anomalous days\n",
            "Finding 3-day consecutive anomalies...\n",
            "Found 0 critical periods (≥3 days)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION REPORT -  firefox windows desktop\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "   Total days:       107\n",
            "   Anomalous days:   16 (15.0%)\n",
            "   Critical periods: 0\n",
            "\n",
            "Analysis complete!\n"
          ]
        }
      ]
    }
  ]
}